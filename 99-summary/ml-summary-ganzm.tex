\documentclass[11pt,twocolumn]{article}

% \documentclass[11pt,conference]{IEEEtran}

%\usepackage{times}
%\usepackage{balance}
\usepackage{url}
\usepackage{xcolor}
\usepackage{graphicx}	% For figure environment
\usepackage[center]{subfigure}
\usepackage{amssymb}	% For mathematical figures like \mathbb{R}
\usepackage{amsmath}
\usepackage{framed}
\usepackage{epstopdf}
\usepackage{geometry}
\geometry{a4paper, top=6mm, left=7mm, right=7mm, bottom=8mm,
headsep=0mm, footskip=0mm}

\begin{document}
\title{ML - Summary 2013}


% Exame summary:
% Two A4-pages (i.e. one A4-sheet of paper), either handwritten or 11 point minimum font size.

\section{Varia}

Generalisation/Prediction Error: Expected \#mistakes on unkown data


$Generalisation Error = Bias^2 + Variance + Noise$	

\section{Probability}

\paragraph{Covariance}: $Cov(X_1, X_2) = E[X_1 X_2] - E[X_1] E[X_2] = E[(X_1 - E[X_1])(X_2-E[X_2])]$

\paragraph{Variance}: $Var[X] = E[X^2] - E[X]^2$


\paragraph{Chain rule}: $P(X,Y) = {{P(X,Y)P(Y)}\over{P(Y)}} = P(X|Y)P(Y)$



\subsection{Gaussian Distribution}

$p_{1D}(x) = {1 \over {\sqrt{2 \pi \sigma^2}}} exp{\left( -{{\left(x - \mu\right)^2} \over{2 \sigma^2}} \right)}$


$p_{dD}(x) = {1 \over {\sqrt{{(2 \pi)}^d |\Sigma|}}} exp{\left( -{ {1 \over 2}  {\left(x - \mu\right)^T} \Sigma^{-1} {\left(x - \mu\right)}}\right)}$


Multiple of Gaussians are gaussian:
$X \sim  \mathcal{N}(\mu, \Sigma)~~, Y = MX \Rightarrow Y \sim \mathcal{N}(M\mu, M \Sigma M^T)$

Sums of Gaussians are gaussian:
$X_1 \sim \mathcal{N}(\mu_1, \Sigma_1)$,
$X_2 \sim \mathcal{N}(\mu_2, \Sigma_2)$, 
$Y=X_1 + X_2$
$\Rightarrow Y \sim \mathcal{N}(\mu_1 + \mu_2; \Sigma_1+\Sigma_2)$





\section{Ridge Regression}

Problem: $ w^* =  arg \min \limits_w \sum \limits_{i=1}^n \left(y_i - w^Tx_i\right)^2 + \lambda ||w||_2^2$

Closed Form: $w^* = \left(X^T X + \lambda I \right)^{-1} X^T y$

\section{Sparse Regression: Lasso}

Problem: $ w^* =  arg \min \limits_w \sum \limits_{i=1}^n \left(y_i - w^Tx_i\right)^2 + \lambda ||w||_1$


\section{Kernelized Linear Regression}

$\min \limits_{\alpha_1,...,\alpha_n} \sum \limits_{i=1}^n\left( \sum \limits_{j=1}^n \alpha_j k(x_i,x_j) - y_i \right)^2 + \lambda \alpha^T K \alpha$

\section{Linear Classification}

Problem: $ w^* =  arg \min \limits_w \sum \limits_{i=1}^n l\left(w;x_i,y_i\right)$


\subsection{Some Loss-Functions}
\textbf{Square Loss} $l_2 = (y_i - w^T x_i)^2$

\textbf{0/1 Loss:} $ l_{0/1}  =  y_i \neq sign(w^Tx_i)$

\textbf{Perceptron Loss:} $l_p\left(w;y_i,x_i \right) =   max(0, -y_i w^T x_i)$

\textbf{Hinge Loss (SVM):} $l_h =   max(0, 1 -y_i w^T x_i)$

\subsubsection{Stochastic gradient Descent}
pick random $x'$ and $y'$
if $ l(w_t;x',y') \neq 0$

$w_{t+1} = w_{t} - \eta \nabla l(w_t;x', y') $  learning rate $\eta$



\section{SVMs}
Hard margin SVM problem: $\min \limits_w w^Tw , s.t.~ y_i W^T x_i >= 1$



Confidence $\eta = y w^T x$

Margin to w-plane $\gamma = \min \limits_{x' \in L} ||x-x'|| $

We are looking for a plane such that every sample has minimum distance 1(not actually 1).


\subsection{Soft margin SVM}

Unconstrained: $\min \limits_w w^Tw + C \sum \limits_i max\left(0, 1-y_iw^Tx_i\right)$


% TODO corrections slide 5 page 4
Constrained: $\min \limits_{w,w_o, e} {{1 \over 2} ||w||^2 + C \sum \limits_{i=1}^n {e_i}}$ 

s.t. $ y_i \left( <w, x_i> + w_o \right) \geq 1 - e_i \forall i = 1,...,n$


Dual Form: $ \max \limits_{\alpha} \sum \limits_{i=1}^m \alpha_i - {1 \over 2} \sum \limits_{i,j} \alpha_i \alpha_j y_i y_j x_i^T x_j$ s.t. $\sum \limits_i \alpha_i y_i  = 0$ and $0 \leq \alpha_i \leq C$


Optimal solution is a linear combination of the data $w^* = \sum \limits_{i=1}^n \left( \alpha_i y_i \right) x_i$

Classification: $y_{new} = sign(  \sum \limits_{i=1}^n \alpha_i y_i  k(x_i, x_{new})  )$

\section{Kernels $k(x,y)$}

$k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}  \Leftrightarrow \forall x,y \in \mathcal{X} : k\left( x , y \right) = k \left(y,x\right)$

$\forall x, y$
Gram Matrix K: $(K)_{i,j} = k(i,j)$ is p.s.d

$k_1(x,y) = \langle \phi_1(x), \phi_1(y) \rangle $

\subsection{Representer Theorem}

Problem: $\min \limits_{f \in \mathcal{H}} \sum \limits_{i=1}^n l(f(x_i);x_i, y_i) + \lambda || f||_{\mathcal{H}}^2$

sol: $\min \limits_{\alpha_1,...,\alpha_n} \sum \limits_{i=1}^n l \left( \sum \limits_{j=1}^n \alpha_j k(x_i, x_j);x_i,y_i  \right) + \lambda \alpha^T K \alpha$

Best $f(x) = \sum \limits_{j=1}^n \alpha_j k(x_j, x)$ is a sum of weighted kernel evaluations.

\paragraph{Gaussian Kernel}: $exp\left( - {{||x-y||^2}\over{2 \sigma^2}} \right)$ 


\paragraph{Sigmoid Kernel}: $tanh(\kappa x^T y + b)$

\paragraph{Polynomial Kernel}: $\left( x^Ty + c \right)^d, c \geq 0$

\paragraph{Closure Properties}

\begin{enumerate}
\item $k(x,y) = a k_1(x,y) + b k_2(x,y)$, $a,b \geq 0$
\item $k(x,y) =  k_1(x,y)  k_2(x,y)$, $k_1, k_2$ are kernels
\item $k(x,y) =  k_3(\phi(x),  \phi(y))$, $k_3$ is a kernel
\item $k(x,y) =  f(x) f(y)$
\item $k(x,y) =  exp(k_1(x,y))$
\end{enumerate}

\section{Multiple Classes}

\subsection{1 VS all}
TODO

\section{Maximum Apposteriory estimation MAP}
Maximize likelihood of model parameter $w^* = arg \max \limits_w P(w|x_1,...x_n,y_1,...,y_n) $ 

map estimate $P(w|x_1,..,x_n,y_1,...,y_n) = {{P(w)P(y_1,...,y_n|x_1,...x_n,w)}\over{P(y_1,...,y_n|x_1,...,x_n)}}$	
	
\section{Bayesian Learning}

\subsection{Prior Assumption}

\paragraph{Laplace} Prior $p(x;\mu, b) = {1 \over {2b}} exp \left( - {{|x - \mu|}\over{b}} \right)$ corresponds to L1-Regularizer

\paragraph{Gauss} Prior $p(x;\mu, \sigma)$ corresponds to L2-Regularizer

\subsection{Logistic Regression}
Classification method which replaces assumption about gaussian noise by iid bernoulli noise.

$P(y|x,w) = Ber(y; \sigma(w^Tx))$


Link Func: $P(Y=+1|x,w) =  \sigma(w^Tx) = 	{1 \over {1+exp(-w^Tx)}}$

Learn $w^* = arg \min \limits_w \sum \limits_{i=1}^n log \left( 1 + exp(-y_iw^Tx_i) \right)$

Classifiy $P(y|x_{new}, w^* ) = {1 \over {1+exp(-y{w^*}^Tx)}}$

\subsection{Bayesian Decision Theory}

If distribution $P(y|x)$ is known, best action $a^*$ is determined: $a^* = arg \min \limits_{a \in \mathcal{A}} E_y[C(y,a)|x]$ where $C(y,a)$ is a cost function.

% see Slide 8



\subsection{Bayesian model Averaging BMA}



% integrate over all models and weight them with their probability
$P(y_{new}, x_{new}, D) = \int  P(y_{new}|x_{new}, w) P(w|D) dw$

\section{Gaussian Processes}

$GP(f; \mu, k)$ with $\mu(x)$ as mean function and $k(x,x')$ as cov. function. If $y_i \sim \mathcal{N}(f(x_i), \sigma^2)$ with $A = \lbrace x_1,...,x_n \rbrace$ then Posterior is a GP $P(f|x_1,...,x_n1,y_1,...,y_n) = GP(f;\mu' , k' )$ with $\mu'(x) = \mu(x) + K _{x,A}(K_{AA}+\sigma^2 I)^{-1}(y A - \mu A)$ and $k' (x,x' ) = k(x,x')  - K_{x,A}(K_{AA} + \sigma^2I )^{-1} K_{A,x'}$


% see Slide 9 page 27

\subsection{Bayesian linear regression}




\section{Ensemble Methods}

\paragraph{Stumps}
$h(x) = sign(ax_i - t), ~a \in \lbrace-1,+1\rbrace $ 

\paragraph{Decision Trees} hierarchical ordered stumps

\subsection{Bagging}
Train each weak learner on a random subset of the datapoints. Classify new points by majority vote.

\subsection{Random Forest classifier}
TODO

\subsection{Boosting}

AdaBoost, with weight $w_i$ per datapoint greedily optimizes for exp loss.
\begin{itemize}
\item for $i=1:m$
\begin{itemize}
\item $h_i \leftarrow arg \min \limits_h \sum \limits_{j=1}^n  w_j^{(i)}[h(x_j) \neq y_j]$
\item $err_i = {{\sum \limits_{j=1}^n w_j^{(i)}[y_j \neq h_i(x_j)]}\over{\sum \limits_{j=1}^n w_j^{(i)}}} $
$,~~\beta_i = log {{1-err_i}\over{err_i}}$

\item $w_j^{(i+1)} = w_j^{(i)} exp(\beta_i [h_i(x_j) \neq y_j])$
\end{itemize}
\item output: $f(x) = \sum \limits_{i=1}^n \beta_i h_i (x)$
\end{itemize}


\section{Generative Models}

\subsection{Conjugate priors}
Pair of Prior assumption $P(y)$ about data and likelihood function is called conjucate if posterior distribution remains in the same family as the prior.	

\section{Unsupervised Learning}
\subsection{K-Means (Lloyd's Algo)}

$z_i \leftarrow arg \min \limits_{j \in \lbrace 1,...,k \rbrace } || x_i - \mu_j^{(t-1)} ||_2^2 $

$ \mu_j^{(t)} \leftarrow {1 \over {n_j}} \sum \limits_{i:z_i = j} x_i$


\subsection{GMM}

$P(x|\theta) = P(x|\mu, \Sigma, w) = \sum \limits_i w_i \mathcal{N} (x;\mu_i, \Sigma_i)$, $~\sum \limits_i w_i = 1$

Latent Variable (apply Bayes): $\gamma_j(x) 
= P(z=j|x,\Sigma, \mu) 
= {{w_j P(x|\Sigma_j, \mu_j)}\over{\sum_j w_j P(x|\Sigma_l , \mu_l)}}$ 


$\mu_j^* = {{\sum_{i=1}^n \gamma_j(x_i)x_i}\over{\sum_{i=1}^n \gamma_j(x_i)}} $ 

$\Sigma_j^* = {{\sum_{i=1}^n \gamma_j (x_i)(x_i - \mu_j^*)(x_i-\mu_j^*)^T}\over{\sum_{i=1}^n \gamma_j(x_i)}} \color{blue} + \nu^2 I$

$w_j^* = {1 \over {n}} \sum_{i=1}^n \gamma_j(x_i)$

\section{LinAlg}

\subsection{Positiv Semidefiniteness (p.s.d)}

$K \in \mathbb{R}$ is psd 

$\iff x^TKx \geq 0 ~\forall x \in \mathbb{R}$


$\iff all eigenvalues of K are \geq 0$


\section{Differentials}

Chain rule: $ f(g(x)) \frac{d}{dx} = f'(g(x)) \cdot g'(x)$

\subsection{Vector/Matrix differentiation}

$\frac{d}{dx} f(x) = \left[ \frac{\partial f }{\partial x_1} ,...,\frac{\partial f}{\partial x_n} \right]$, $\frac{d}{dx}(b^T x) = \frac{d}{dx} (x^T b) = b$,$\frac{d}{dx}(x^T x) = \frac{d}{dx} (x^T x) = 2x$,$\frac{d}{dx}(x^T A x) = (A^T + A) x$


\section{Convex optimisation}

minimize $f(x)$ subject to 

$g_i(x) \leq 0, i = 1,...,m$ inequality constr.

$h_i(x) = 0, i = 1,...,p$ equality constr.

Create the Lagrangian

$L(x, \lambda, \nu) = f(x) + \sum_{i=1}^m{\lambda_i g_i(x) + \sum_{i=1}^p{\nu_i h_i(x)}}$

Lagrange dual function: $ d(\lambda, \nu) = \inf_{x} L (x,\lambda, \nu) $

Lagrange dual problem: max. $d(\lambda, \nu)$  subj. to $\lambda \geq 0$



\maketitle


%% Memo - ignore this

\section{PCA - Principal Component Analysis}

High dimensional data is projected onto a low dimensional subspace while maximizing variance.

\begin{itemize}
\item project $x$ to $\tilde{x}$ and minimize error $|| x_n - ~x_n ||_2$
\item variance of projected data is maximized
\end{itemize}

\subsubsection{Algorithm}

\begin{itemize}
% \item $ X = [x_1,x_2,...x_n]$
\item Covariance $\Sigma = \frac{1}{N} \cdot (  X - M ) ( X - M )^T$
\begin{itemize}
\item $Cov(X_i, X_i) = Var(X_i)$
\item Symmetric: $ Cov(X_i,X_j) = Cov(X_j, X_i)$
\end{itemize}
\item $Eig(\Sigma) = U \cdot \Lambda \cdot U^T$
% \item Select first k orthonormal vectors from U and get projection matrix. 
\item $Z = U_k^T \cdot X$ where $Z$ is dim reduced.

\end{itemize}

\subsubsection{Deduction} 
Var. of proj. data $Z$ is maximal if cov.

$\Sigma_Z = A^T \Sigma_X A = \frac{1}{n} (A^TX - \bar{X})(A^TX - \bar{X})^T $.

By choosing $A = U$ where $\Sigma_X = U \Lambda U^T$ the covariance $\Sigma_Z$ becomes diagonal.

%\subsubsection{Applications} 
%Compress $X$ with $U_k$ principal components. 
%Project X to a k-dimensional subspace with $Z = U_k \cdot X$

\section{SVD $M = U D V^T$}

\begin{itemize}

%% Dimensions of svd matrices
%\item Dimensions (M: $m \times n$)
%	\begin{itemize}
%	\item U: $m \times m$
%	\item D: $m \times n$
%	\item V: $n \times n$
%\end{itemize}

\item Rank of $M$: Number of singular values
\item Null space: right columns of $V$ where $\sigma_i$ are 0
\item Range of $M$: left columns of $U$ where $\sigma_i$ are $\neq 0$
\item Pseudo-Inverse: $M^+ = U D^+ V$, where $D^+ = D$ with inverted singular values

\end{itemize}

\subsubsection{SVD as a sum}
%$ M = \sum_{i=1}^{N}{ U_i \cdot \Sigma_i \cdot V^T_i } %$

$ M_k = \sum_{i=1}^{k} {U_i \cdot \Sigma_i \cdot V^T_i} $
 
\textbf{Minimize L2 Norm:}
SVD solves $|| M - B ||_2 = || M - M_k ||_2$
for euclidean matrix norms

\subsection{Important} Eigenvectors of $MM^T$ and $M^TM$:


$MM^T = UDV^T VDU^T = UD (V^T V)DU^T = U D^2 U^T$
$M^TM =  ....  = V D^2 V^T$
 
If $M = M^T$ (symmetric and real) then
$S = U \cdot D \cdot U^T$
Where $U$ has columns of Eigenvectors


\section{Linear Algebra}
\subsection{Vector Norms}
are positive scalable, full-fill the triangular inequality, norm of 0 is 0

\subsubsection{p-Norm}
$ || x ||_p = \left( \sum_{i=1}^{n}{|x_i|^p} \right)^{\frac{1}{p}}$

\subsubsection{Euclidean Norm} 
p-Norm where $p=2$

\subsubsection{1-Norm}
Manhattan-Norm
$ ||x||_1 = \sum_{i=1}^{n}{|x_i|} $

\subsubsection{Zero-Norm} 
counts the number of non-zero entries.

\subsection{Matrix Norms}

\subsubsection{Nuclear Norm}

$|| . ||_*$ sum of singular values

\subsubsection{Frobenious-Norm}
$sqrt(sum(sum(A.^2)))$

\subsubsection{Spectral Norm} 
Largest singular value if square 

$||A||_2 = \sigma_{max}(A)$ 
~~Is equals to the 2-Norm

\subsubsection{Induced Matrix Norms}
$ ||A|| = max \left( \frac{ ||Ax|| }{ ||x|| } \right)$

\subsection{Orthogonality}

\subsubsection{Vectors} 

inner (scalar) product $\langle ~.~,~.~ \rangle = 0$

\subsubsection{Matrices} 

quadratic, values are in $\mathbb{R}$, $Q^T = Q^{-1}$

\subsubsection{Functions}

$f(x)$ orth. to $g(x)$ if $0 = \int f(x) g(x) dx $

\subsubsection{Coherence}

$m(U)= max_{i,j:i\neq j} | u_i^T u_j|$

\subsubsection{Convexity}

$f(\theta x + (1-\theta)y) \leq \theta f(x) + (1-\theta) f(y)$

\section{Differentials}

Chain rule: $ f(g(x)) \frac{d}{dx} = f'(g(x)) \cdot g'(x)$

\subsection{Vector/Matrix differentiation}

$\frac{d}{dx} f(x) = \left[ \frac{\partial f }{\partial x_1} ,...,\frac{\partial f}{\partial x_n} \right]$, $\frac{d}{dx}(b^T x) = \frac{d}{dx} (x^T b) = b$,$\frac{d}{dx}(x^T x) = \frac{d}{dx} (x^T x) = 2x$,$\frac{d}{dx}(x^T A x) = (A^T + A) x$

\section{Probability}

\subsection{Notation}
$Pr\lbrace ... \rbrace$ Probability of an event

$P(x)$ Probability mass function (Verteilungsfunktion)

$p(x)$ Probability density function (Dichtefunktion)

$P(X,Y) = P(X|Y) \cdot P(Y) = P(Y|X) \cdot P(X)$

Bayes: $P(X|Y) = \frac{P(Y|X)\cdot P(X)}{P(Y)}$


\section{Collaborative Filtering with SVD}
Init/Set values to predict in $M$ to be the avg value.

$M = U \cdot D \cdot V^T$

U = Row-to-Concept affinity

V = Column-to-Concept affinity

D = expressiveness of each concept in the data

\subsection{Add new row (User Bob)}

$M_{Bob} = U_{Bob} \cdot D \cdot V^T => M_{Bob} \cdot V \cdot D^-1 = U_{Bob}$


\section{K-Means $X = U \cdot Z$}

%\begin{itemize}
%\item X: $D \times N$
%\item U: $D \times k$
%\item Z: $k \times N$
%\end{itemize}

\subsection{Hard Assignment}

Minimize cost function:
$J(U,Z) = || X - UZ ||^2_f = \sum_{n=1}^{N}{\sum_{k=1}^{K}{z_{k,n} ||x_n - u_k||_2^2}}$


Equ. holds only iff $z_{k,n}$ is boolean and $sum(Z_n) = 1$

\subsubsection{Algorithm}

Step 1: Cluster Assignment
Hard-Assign to Cluster where
$ || X_n - U_i ||^2_2$ is minimal

\footnotesize

$ k^*(x_n) = argmin \lbrace ||x_n-u_1||_2^2,...,||x_n-u_k||_2^2,...,||x_n-u_K||_2^2 \rbrace$
\normalsize

Step 2: Centroid update $u_k$: 
Sum up data points associated to k-th centroid and average.

$u_k = \dfrac{\sum_{n=1}^N{z_{k,n} \cdot x_n}}{\sum_{n=1}^N{z_{k,n}}} $

\subsubsection{Convergence K-Means}

Step 1 minimizes J because it sets $z_k,$ where $ || X_n - U_i ||^2_2$ is minimal

Step 2 minimizes J because $u_k = \dfrac{\sum_{n=1}^N{z_{k,n} \cdot x_n}}{\sum_{n=1}^N{z_{k,n}}} $ is the derivative of $J$ with respect to $u_k$:

$\dfrac{\partial{J}}{\partial{u_k}} = \dfrac{\partial \sum_{n=1}^N{z_{k,n}} ||x_n - u_k||_2^2}{\partial{u_k}} = \sum_{n=1}^N{z_{k,n}} {\left[  	\dfrac{\partial{(x_{1,n}-u_{1,k})^2}} 	{\partial u_{1,k}}, ...,\dfrac{\partial{(x_{d,n}-u_{d,k})^2}} 	{\partial u_{d,k}}  \right]}^T = -2 \sum_{n=1}^N{z_{k,n}(x_n - u_k)} => $  solve for $u_k$
$, \dfrac{\partial{J^2}}{u_k^2} > 0 => J$ does not increase after centroid update. 

\subsection{Estimate K - $\kappa(.)$ num. free param.}
\subsubsection{AIC} 
$= - ln p(X|.) +\kappa(U,Z)$

\subsubsection{BIC} 
$= - ln p(X|.) + \frac{1}{2} \kappa(U,Z) ln(N)$

\subsection{EM with GMM Gaussian Mixture Model}
\begin{itemize}
\item $p(x) = \sum_{k=1}^{K}{\pi_k~~ p(x|\theta_k)}$
\item $\sum_{k=1}^K{\pi_k}=1$ Each column sums up to 1
\item Gaussian Distr.: $\mu$: Expectation, $\sigma^2 = variance$, $\sigma = stddev$

\item introduce latent variable $\gamma$ in the E-Step and marginalize away in the M-Step
\item $\gamma(z_{k,n})$ is the prob. of $x_n$ beeing ass. to cluster $k$

\end{itemize}

\subsubsection{E-Step} Evaluate Responsibilities

$\gamma(z_{k,n}):=\mathbb{E}[z_{k,n}] = \frac{\pi_k \mathcal{N}(x_n|\mu_k, \Sigma_k)}{\sum_j \pi_j \mathcal{N}(x_n|\mu_j, \Sigma_j)}$


\subsubsection{M-Step} Re-Estimate model parameters

$N_k = \sum_{n=1}^N{\gamma(z_{k,n})}$

$u_k^{new} = \frac{1}{N_k} \sum_{n=1}^N{\gamma(z_{k,n})x_n}$, $\pi_k^{new}= \frac{N_k}{N}$


\section{Non-negative matrix factorisation}

$X \in \mathbb{R+}^{D}$ 
 
Similar to k-means:
\begin{enumerate}


\item Init $U$, $Z$ (random positive values)
\item Iterate
\item Update $U$:
$\tilde{X} = UZ$
$X = UZ => XZ' = UZZ'$
$XZ'/UZZ'$ is a coefficient matrix in $\mathbb{R^+}$ 


$u_dk_{new} = u_dk \cdot ( (XZ')/(UZZ') )$

\item Update $Z$: $X = UZ$;  $U'X = U'UZ$; $z_dk_{new} <- z_dk * ( (UX)/(U'UZ) )$

\subsection{Deduction}

${\min\limits_{U,Z}} ~~ J(U,Z) = \frac{1}{2} ||X-UZ||_F^2 = \frac{1}{2} tr\left( (X-UZ)(X-UZ)^T \right)$

Lagrangian $L(U,Z,\alpha, \beta)=J(U,Z)-tr(\alpha U^T) - tr(\beta Z^T)$

where tr(.) is the trace of a matrix

\end{enumerate}
\subsection{Kullback-Leibler Divergence}
$D_{KL}(P||Q) = \sum_{x \in X} P(x) \log \left( \dfrac{P(x)}{Q(x)}\right)$
KL-divergence of $X$ and $UZ$ for pLSI:
$\min\limits_{U,Z}\sum_{d=1}^D\sum_{n=1}^N x_{dn}\log\left(\dfrac{x_{dn}}{(UZ)_{dn}}\right)$ s.t. $ \sum_{d=1}^D u_{dk} =1 \forall k, \sum_{d,n}z_{kn}=1, u_{dk}\geq0,z_{á¸±n}\geq0$
\section{Role Based Access control - RBAC}
Model with $\beta = (p\left\lbrace u_{dk}=0\right\rbrace)^{D \times K}$

SAC: $p(X|\beta,Z) = \prod\limits_{n,d}(1-\beta_{dk_n})^{x_{dn}}(\beta_{dk_n})^{(1-x_{dn})}$

MAC: $p(X|\beta,Z) = \prod\limits_{n,d}(1-\prod\limits_k \beta_{dk}^{z_{kn}})^{x_{dn}}(\prod\limits_k\beta_{dk}^{z_{kn}})^{1-x_{dn}}$

Coverage:$Cov:=\dfrac{|\left\lbrace(i,j)|\hat{x}_{i,j}=x_{i,j}=1\right\rbrace|}{|\left\lbrace(i,j)|x_{i,j}=1 \right\rbrace|}$

Deviating Ones:$d1:=\dfrac{|\left\lbrace(i,j)|\hat{x}_{i,j}=x_{i,j}=1,x_{i,j}=0\right\rbrace|}{|\left\lbrace(i,j)|x_{i,j}=1 \right\rbrace|}$

Deviating Zeros:$d0:=\dfrac{|\left\lbrace(i,j)|\hat{x}_{i,j}=x_{i,j}=0,x_{i,j}=1\right\rbrace|}{|\left\lbrace(i,j)|x_{i,j}=0 \right\rbrace|}$

\section{Compressive Sensing}

\begin{itemize}
\item $x$ is a D-Dimensional measurement
\item $x$ is sparse in some orthonormal basis $U$, $x = U \cdot z$
\item instead of saving $x$ we save $y$ with dim. $M<<D$
\item define any orthonormal basis $U$ $(D \times D)$
\item define W $(M \times D)$
\item $y = Wx = WUz := \Theta z$

\item $\Theta = W \cdot U$
\item Store y: $Wx => y$
\item Restore x: $y = \Theta \cdot z$, find most sparse matrix $z$
	\begin{itemize}
	\item arg min z :  $||z||_0$  s.t.  $\Theta z  = y$ (matching persuit)
	\item $x = U \cdot z$
  	\end{itemize}
\end{itemize}

\section{Sparse Coding}
\subsubsection{Matching Pursuit}

Exact Recovery Conditions

$K< \frac{1}{2} \left(1+\frac{1}{m(U)}\right)$

where Coherence:  $m(U)= {max  \atop i,j:i \neq j} ~~ |u_i^T u_j|$

\subsubsection{Overcomplete Dicts.}
\begin{itemize}
\item increasing overcompleteness
\item increases (potentially) to a certain point sparse coding (gets sparser)
\item increases linear dependence between atoms
\item Solve: $arg min ||z||_0 s.t. x = Uz$

\end{itemize}



%----------------------------------------
%Dictionary Learning
%----------------------------------------  
\section{Dictionary Learning}
$X = U \cdot Z$ alternate betw.
Coding and Dict. update step
\begin{itemize}
\item Update $Z$ to be as sparse as possible (with MP)
\end{itemize}

Dictionary Update Step
\begin{itemize}
\item  $U_{new} =  arg min~ U  ||X-UZ||^2_F$
\item Update one dictionary item $U_l$ at a time
	\begin{itemize}
	

  \item write $U \cdot Z$ as sum omit index l: 
  $\sum_{i \neq l}{U_i \cdot Z^T_i}$
  \item Residual $R_l =  X - \left(\sum_{i \neq l}{U_i \cdot Z^T_i}\right)$
  \item $=> R_l = U_l \cdot Z^T_l$ (where $R_l$, $Z^T_l$ fix)
  \item $R_l = UDV^T$ update $U_l$ with first column of $U$
	\end{itemize}  
   (hint: write SVD as SUM and you will see)

\end{itemize}


%----------------------------------------
%Robust PCA R-PCA
%----------------------------------------
\section{Robust PCA R-PCA}
$X = L + S$  ($L$ is low rank, $S$ is sparse)

relax the problem to:

minimize 	$||L||_* + \lambda \cdot ||S||_1$
subject to 	$L+S = X$

\subsection{Convex optimisation}

minimize $f(x)$ subject to 

$g_i(x) \leq 0, i = 1,...,m$ inequality constr.

$h_i(x) = 0, i = 1,...,p$ equality constr.

Create the Lagrangian

$L(x, \lambda, \nu) = f(x) + \sum_{i=1}^m{\lambda_i g_i(x) + \sum_{i=1}^p{\nu_i h_i(x)}}$

Lagrange dual function: $ d(\lambda, \nu) = \inf_{x} L (x,\lambda, \nu) $

Lagrange dual problem: max. $d(\lambda, \nu)$  subj. to $\lambda \geq 0$

\subsection{ADMM - Alternating Direction Method of Multipliers}


\subsubsection{Alternate Direction}


Lagrangian: $L(x,\nu)$

Dual Function: $d(\nu)=\inf_x L(x,\nu)$

Dual Problem: maximize $d(\nu)$

Recover optimal x: $x^* \in argmin_x L(x, \nu^*)$

Gradient Method: $\nu^{k+1} = \nu^k + \alpha^k \nabla d(\nu^k)$

$\nabla  d (\nu^k) = f(\tilde{x})$ , ~~ where
$\tilde{x} = argmin_x L(x, \nu^k)$

\subsubsection{Dual decomposition}

\begin{enumerate}

\item if $f(x)$ is separable into $f_1(x_1) + f_2(x_2) + ... + f_n(x_n)$
	  then $L(x,v)$ is separable
	  so we can split the x-minimisation step
\item Method of multipliers
\item create augmented lagrange by adding a penalty function $\frac{\rho}{2}|| .||^2_2$
\item add more penalty for violating constraints, leads to convergence under far more general condition
	
\end{enumerate}


\subsubsection{ADMM in short}

minimize $f(x) + p(z)$ s.t. $Ax+Bz=c$
Augm. Lagrange: $L_p(x,z, \nu)= f(x) +p(z)+\nu^T (Ax+Bz-c) + \frac{\rho}{2}  ||Ax+Bz-c||_2^2$

ADMM:

$x^{k+1} := argmin_x L_{\rho} (x, z^k, \nu^k)$

$z^{k+1} := argmin_z L_{\rho} (x^{k+1}, z, \nu^k)$

$\nu^{k+1} := \nu^k + \rho  (Ax^{k+1} + Bz^{k+1} -c)$

\subsubsection{PCP Recovery Condition}

Probability. $ 1 - \mathcal{O}(n^{-10})$ with $\lambda = \frac{1}{sqrt(n)}$

\end{document}
