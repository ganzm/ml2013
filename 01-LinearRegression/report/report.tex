\documentclass[a4paper, 11pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[pdftex]{hyperref}

% used to list matlab code - check mcode.zip mcode.sty
\usepackage[framed,numbered,autolinebreaks,useliterate]{mcode}


% Lengths and indenting
\setlength{\textwidth}{16.5cm}
\setlength{\marginparwidth}{1.5cm}
\setlength{\parindent}{0cm}
\setlength{\parskip}{0.15cm}
\setlength{\textheight}{22cm}
\setlength{\oddsidemargin}{0cm}
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\topmargin}{0cm}
\setlength{\headheight}{0cm}
\setlength{\headsep}{0cm}

\renewcommand{\familydefault}{\sfdefault}

\title{Machine Learning 2013: Project 1 - Regression Report}
\author{fregli@student.ethz.ch\\ ganzm@student.ethz.ch\\ sandrofe@student.ethz.ch\\}
\date{\today}

\begin{document}
\maketitle

\section*{Experimental Protocol}
\label{sec:exp-protocl}
%Suppose that someone wants to reproduce your results. Briefly describe the steps used to obtain the predictions starting from the raw data set downloaded from the project website. Use the following sections to explain your methodology. Feel free to add graphs or screenshots if you think it's necessary. The report should contain a maximum of 2 pages.

This project was performed using Matlab only. To reproduce test results presented in this report the following steps have to be taken:

\begin{itemize}
\item Unzip the sourcefolder containing Matlab code and data sets
\item Run the function learn() located in learn.m to obtain both files \textit{testresult.csv} and \textit{validationresult.csv}
\end{itemize}

\section{Tools}
%Which tools and libraries have you used (e.g. Matlab, Python with scikit-learn, Java with Weka, SPSS, language x with library y, $\ldots$). If you have source-code (Matlab scripts, Python scripts, Java source folder, \dots), make sure to submit it on the project website together with this report. If you only used command-line or GUI-tools describe what you did.

As stated in Section \nameref{sec:exp-protocl} the only tool which is needed is Matlab - no fancy special commands, no additional libraries are required.

\section{Algorithm}
%Describe the algorithm you used for regression (e.g. ordinary least squares, ridge regression, $\ldots$)

The algorithm performs the following steps an can be started by running learn().

\paragraph{Read csv files}
All 3 files read in a first step.

\paragraph{Remove outliers}
We explicitly remove training data which we have detected to affect our learning algorithm in a negative way.

\paragraph{Split training set}
Training data are split into subsets so that each of them contains the measurement value for one specific \textbf{L2 Cache size}.

\paragraph{Choose features}
In a next step the relevant features are selected for each subset of the data.

\paragraph{Crossvalidation}
For each of the subset we use crossvalidation to determine the optimal parameter $\lambda$ and weight vector $w$ to use for ridge regression.

As a result of this step we get an optimal $\lambda$ and $w$ for each observed \textbf{L2 Cache size} value.

\paragraph{Compute result}
To estimate the values for the test and the validation set we check the cache size of each individual record and then choose which weight vector to apply.

\subsection{Which part of the code does what}

\paragraph{learn.m}
This is the main script which can be run to obtain the submitted results.
\paragraph{trainData.m}
Performs Ridge Regression and solves the following problem

$ \min \limits_w \sum \limits_{i=1}^n \left(y_i - w^Tx_i\right)^2 + \lambda ||w||_2^2$

using the exact solution:


$ w = \left(X^T X + \lambda I   \right)^{-1} X^T y$

As a result we obtain the weight vector $w$ and an the RMSE.

\paragraph{crossvalidation.m}

Performns crossvalidation to determin the optimal parameter $\lambda$.

\section{Features}

As mentioned the training set was split according to \textbf{L2 Cache} sizes. We then observed that the correlation between the resulting delay values and the \textbf{depth} feature was very good. Similar behaviour was observed choosing the following to more features

\begin{itemize}
\item  \textbf{LSQ size} $\cdot$  \textbf{branches allowed}

\item \textbf{RF size} $\cdot$ \textbf{branches allowed}
\end{itemize}

By splitting the training set into subsets of \textbf{L2 Cache size} we achieve quite good results with only 3 very simple features.



% Did you construct any new features? What feature transforms did you use?


\subsection{Find good features}
\label{subsec:findgoodfeatures}

We had different approaches to find good featues:

\begin{itemize}
	\item \textbf{I'm feeling lucky} Simply plotting each feature and some handcrafted combinations against the delay in order to see if and how they correlate. This was an annoying task, since it took long time and at the end it revealed only very few features.

	\item \textbf{Brute force automation:} Create feature combinations automatically and just look at the correlation matrices. This was easy to compute and it took only a few seconds to verify the results (See listing below). With this method we obtained a lot more useful features.
	\begin{center}
	\begin{lstlisting} 
	x = trainingData(:,1:14);
	y = transform(trainingData(:,15));

	% prepare features
	xfeatures = [y, x, x.^2, x.^3, x.^4, sqrt(x), log2(x)];

	% calculate correlation
	corr = corr(xfeatures, xfeatures);

	% only consider how good the y feature correlates
	c = corr(:,1);
	\end{lstlisting}
	Figure 1: Code Sample to find correlation
	\end{center}

\end{itemize}

\section{Lessons Learned} 
Most of the time spent for the project was used to search for good features. At the start of the project  it was mostly unclear how to find such features. Getting a test score which meets our expectation by simply try and error features did not yield the expected result.

We soon automated our search for good features by correlating the resulting delay vector to any kinds of transformations of our input data.

While observing plots we found out that the \textbf{Depth} feature looks very linear when considering \textbf{L2 Cache size}. This lead us to the solution we are proposing.

All in all we learned that machine learning requires quite a bit of creativity.

%What other algorithms, tools or methods did you try out that didn't work well?
%Why do you think they performed worse than what you used for your final submission?

\end{document} 
